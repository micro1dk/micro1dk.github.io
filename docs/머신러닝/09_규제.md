---
title: 규제
layout: default
parent: 머신러닝
nav_order: 15
---

## 규제

지금까지 다룬 회귀모델의 Cost Function은 **RSS**(Residual Sum of Squares)를 최소화하는 것이다. 

앞의 예제로 차수가 15인 상황에서는 회귀계수가 매우 크다. 따라서 과적합이 발생한다.

차수가 높은 회귀모델에서 회귀 계수를 낮추는 방법으로 **규제(Regularization)**기법이 사용된다. 규제는 계수의 크기에 패널티(벌점)를 부과하여 모델의 복잡성을 줄이고 과적합을 방지한다.

주로 사용되는 두 가지 규제가 있다,

* **릿지 회귀(Ridge Regression)**: 계수의 제곱합에 대해 벌점을 부과한다. 릿지 회귀는 계수의 크기를 줄이되, 완전히 0으로 만들지는 않는다.
* **라쏘 회귀(Lasso Regression)**: 계수의 절대값에 대해 벌점을 부과한다. 라쏘 회귀는 계수를 0으로 만들 수 있어, 특정 특성이 모델에 미치는 영향을 완전히 제거시킨다. 이렇게 특성이 제외되는 것을 **Feature Selection**의 한 형태로 볼 수 있다.



Ridge와 Lasso를 시각적으로 잘 설명하는 좋은 영상이 있다.

[유튜브 영상링크](https://youtu.be/Xm2C_gTAl8c?si=_TdAWPFOd6tvY0H7)





## 규제 비용함수 목표

L2 규제 예시

![](../../assets/images/ml/ridge1.png)

비용함수의 목표는 Cost가 최소화되는 방향으로 가야한다. 규제 하이퍼파라미터 람다의 값에 따라 아래 특징을 가진다.

* **λ = 0** : 비용함수는 기존과동일. 규제를 적용하지 않음
* **λ 증가** : 값이 너무 커지게 되므로 회귀계수를 작게하여 Cost가 최소화되는 목표를 달성할 수 있음. 하지만 너무  높은 값이면 회귀계수가 더 작아지므로 중요한 패턴을 포착하지 못하고 **과소적합을 유발**할 수 있다.
* **λ 감소** :  회귀계수에 대한 제한이 줄어든다. 모델이 복잡한 패턴을 잘 포착할 수 있지만, 너무 낮은 값은 회귀계수가 더 커지므로 **과대적합을 유발**할 수 있다.



### L2 규제 - Ridge

계수의 제곱합에 대해 벌점을 부과한다.

![](../../assets/images/ml/ridge1.png)

![](../../assets/images/ml/ridge2.png)

모든 특성을 유지하면서 계수의 크기를 줄인다. 이로 인해 중요하지 않은 특성의 계수도 완전히 0이 되지는 않는다.

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_diabetes
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

# 데이터셋 로드
diabetes = load_diabetes()

X, y = diabetes.data, diabetes.target

# 릿지 회귀 모델 생성
ridge = Ridge(alpha=0.1)

# 교차 검증을 사용한 모델 성능 평가
neg_mse_scores = cross_val_score(ridge, X, y, scoring='neg_mean_squared_error', cv=5)
rmse_scores = np.sqrt(-neg_mse_scores)

# 결과 출력
avg_rmse = np.mean(rmse_scores)

print(' 5 folds 의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 3))
print(' 5 folds 의 개별 RMSE scores : ', np.round(rmse_scores,3))
print(' 5 folds 의 평균 RMSE : {0:.3f} '.format(avg_rmse))
```

```
 5 folds 의 개별 Negative MSE scores:  [-2869.027 -3054.998 -3178.162 -2935.592 -2995.75 ]
 5 folds 의 개별 RMSE scores :  [53.563 55.272 56.375 54.181 54.733]
 5 folds 의 평균 RMSE : 54.825 
```

당뇨데이터셋에서는 alpha값이 1일 때 RMSE는 54를 출력하였다.

alpha값에 따른 컬럼별 회귀계수를 출력해보자면

```python
ridge_alphas = [0 , 0.1 , 1 , 10 , 100]
sort_column = 'alpha'+str(ridge_alphas[0])
coeff_df = pd.DataFrame()

for pos , alpha in enumerate(ridge_alphas) :
    ridge = Ridge(alpha = alpha)
    ridge.fit(X , y)
    coeff = pd.Series(data=ridge.coef_, index=diabetes.feature_names)
    colname='alpha'+str(alpha)
    coeff_df[colname] = coeff
  
coeff_df.sort_values(by=sort_column, ascending=False)
```

```
	alpha0	alpha0.1	alpha1	alpha10	alpha100
s5	751.273700	443.812917	262.944290	70.143948	8.876851
bmi	519.845920	489.695171	306.352680	75.416214	9.240720
s2	476.739021	-70.826832	-29.515495	13.948715	2.616766
bp	324.384646	301.764058	201.627734	55.025160	6.931289
s4	177.063238	115.712136	117.311732	48.259433	6.678027
s3	101.043268	-188.678898	-152.040280	-47.553816	-6.174550
s6	67.626692	86.749315	111.878956	44.213892	5.955597
age	-10.009866	1.308705	29.466112	19.812842	2.897090
sex	-239.815644	-207.192418	-83.154276	-0.918430	0.585254
s1	-792.175639	-83.466034	5.909614	19.924621	3.230957
```

알파가 높아질 수록 회귀계수가 낮아지는것을 확인했다.





### L1 규제 - Lasso

계수의 절댓값에 대해 벌점을 부과한다. 릿지는 회귀계수를 감소시키지만 라쏘는 회귀계수를 0으로 만든다.

![](../../assets/images/ml/lasso1.png)



![](../../assets/images/ml/lasso2.png)

중요하지 않은 특성의 계수를 완전히 0으로 만든다. Feature Selection의 효과가 있다. 이는 모델의 해석을 더 쉽게 만들어 준다.

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_diabetes
from sklearn.linear_model import Lasso
from sklearn.model_selection import cross_val_score

# 데이터셋 로드
diabetes = load_diabetes()

X, y = diabetes.data, diabetes.target

# 라쏘 회귀 모델 생성
lasso = Lasso(alpha=1)

# 교차 검증을 사용한 모델 성능 평가
neg_mse_scores = cross_val_score(lasso, X, y, scoring='neg_mean_squared_error', cv=5)
rmse_scores = np.sqrt(-neg_mse_scores)

# 결과 출력
avg_rmse = np.mean(rmse_scores)

print(' 5 folds 의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 3))
print(' 5 folds 의 개별 RMSE scores : ', np.round(rmse_scores,3))
print(' 5 folds 의 평균 RMSE : {0:.3f} '.format(avg_rmse))
```

```
 5 folds 의 개별 Negative MSE scores:  [-3491.742 -4113.86  -4046.911 -3489.752 -4111.927]
 5 folds 의 개별 RMSE scores :  [59.091 64.139 63.615 59.074 64.124]
 5 folds 의 평균 RMSE : 62.009 
```

Ridge보다는 높은 오차를 확인할 수 있다.

```python
lasso_alphas = [0 , 0.1 , 1 , 10 , 100]
sort_column = 'alpha'+str(lasso_alphas[0])
coeff_df = pd.DataFrame()

for pos , alpha in enumerate(lasso_alphas) :
    lasso = Lasso(alpha = alpha)
    lasso.fit(X , y)
    coeff = pd.Series(data=lasso.coef_, index=diabetes.feature_names)
    colname='alpha'+str(alpha)
    coeff_df[colname] = coeff
  
coeff_df.sort_values(by=sort_column, ascending=False)
```

```
	alpha0	alpha0.1	alpha1	alpha10	alpha100
s5	751.273690	483.912648	307.605418	0.0	0.0
bmi	519.845920	517.186795	367.703860	0.0	0.0
s2	476.739000	-0.000000	0.000000	0.0	0.0
bp	324.384645	275.077235	6.298858	0.0	0.0
s4	177.063234	0.000000	0.000000	0.0	0.0
s3	101.043256	-210.157991	-0.000000  -0.0 -0.0
s6	67.626692	33.673965	0.000000	0.0	0.0
age	-10.009866	-0.000000	0.000000	0.0	0.0
sex	-239.815644	-155.359976	-0.000000	0.0	0.0
s1	-792.175612	-52.539365	0.000000	0.0	0.0
```

알파값이 증가함에 따라 일부 회귀계수가 0이된다. 이는 라쏘회귀가 규제를 강화함에 따라 중요하지 않은 특성의 계수를 제거한다. 알파값이 100일 때, 대부분의 계수가 0이다. 모델이 대부분의 특성을 무시한다.



### L1 + L2 규제 - ElasticNet

라쏘회귀에서는 중요하지 않은특성들을 제거하는 역할을 했지만 사실 그 특성들이 조금씩은 역할을 할 때가 있다.

중요하지 않은 특성을 제거했으니 남은 특성에 대한 의존성이 증가하다보니, 특성들의 회기계수가 상대적으로 변동성이 심해진다. 이를 보완하기 위해 L1규제와 L2 규제를 결합한 엘라스틱넷이 등장하였다.

![](../../assets/images/ml/ela1.png)

알파의 값에 따라 알파가 0이면 L2규제를, 알파가 1이면 L1규제를 적용시키며, 0과 1 사이값이면 L1과 L2규제를 적절히 적용시킨다.

 ```python
from sklearn.linear_model import ElasticNet

elasticnet_alphas = [ 0.07, 0.1, 0.5, 1, 3]
sort_column = 'alpha'+str(elasticnet_alphas[0])
elasticnet_coeff_df = pd.DataFrame()

for alpha in elasticnet_alphas:
    elasticnet = ElasticNet(alpha=alpha, l1_ratio=0.7)
    elasticnet.fit(X, y)
    coeff = pd.Series(data=elasticnet.coef_, index=diabetes.feature_names)
    colname = 'alpha' + str(alpha)
    elasticnet_coeff_df[colname] = coeff

elasticnet_coeff_df.sort_values(by=sort_column, ascending=False)
 ```

```
alpha0.07	alpha0.1	alpha0.5	alpha1	alpha3
bmi	78.372123	57.938119	11.544017	4.744439	0.053248
s5	72.720569	54.278589	10.997870	4.487781	0.000000
bp	56.534960	42.039222	8.079540	2.991006	0.000000
s4	49.239404	37.750621	7.694049	2.837828	0.000000
s6	44.919698	34.082378	6.608643	2.266564	0.000000
s1	19.039090	15.192917	2.527620	0.205676	0.000000
age	18.908138	14.496770	2.029207	0.000000	0.000000
s2	12.618259	10.650735	1.604738	0.000000	0.000000
sex	-0.000000	0.000000	0.000000	0.000000	0.000000
s3	-48.441153	-36.339227	-6.932461	-2.417486	-0.000000
```

알파가 증가됨에 따라 회귀계수의 변화를 관찰하자. 0이면 중요하지 않다고 판단한 것이다.