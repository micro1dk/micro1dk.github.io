---
title: 회귀
layout: default
parent: 머신러닝
nav_order: 11
---

# 회귀

회귀는 두 변수 사이의 관계를 모델링하는 방법이다. 일반적으로는 하나 이상의 독립 변수 (예측 변수 또는 입력 변수)와 종속 변수 (타겟 변수 또는 출력 변수) 사이의 관계를 설명한다.

선형회귀의 종류에는

- 일반선형회귀 : 예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소화할 수 있도록 회귀 계수를 최적화 하며, 규제를 적용하지 않음
- Ridge : 선형회귀에 L2규제를 적용
- Lasso : 선형회귀에 L1규제를 적용
- ElasticNet : L1, L2 함께 적용
- Logistic Regression : 사실은 분류에서 사용되는 선형모델. Sigmoid함수의 반환 값을 확률로 간주해 확률에 따라 분류를 결정한다는 것이다.





## LinearRegression

예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소화하는 OLS(Ordinary Least Squares) 추정 방식으로 구현한 클래스.

주요 입력파라미터는

* **fit_intercept (default=True)**
  * False일시 모델은 원점을 지나는 선을 찾는다.
* **n_jobs (default=None)**
  * 



fit 메서드로 X, y배열을 입력받아 W(회귀계수)를 coef_ 속성에 저장한다.

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np

# 예시 데이터 생성
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Linear Regression 모델 생성 및 학습
model = LinearRegression(fit_intercept=True)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)

print("Intercept (y-절편):", model.intercept_)
print("Coefficient (기울기):", model.coef_)
print("Mean Squared Error (MSE):", mse)
```

```
Intercept (y-절편): [4.14291332]
Coefficient (기울기): [[2.79932366]]
Mean Squared Error (MSE): 0.6536995137170021
```



## Multicollinearity

다중공선성(Multicollinearity)은 선형회귀모델에서 **독립 변수들 간에** **강한 상관관계**가 있을 때 발생하는 문제다. 피쳐간의 상관관계가 매우높은 경우 분산이 커져서 오류에 민감하다. 이를 해결하기 위한 방법으로 Feature Selection, 차원축소, 릿지, 라쏘와 같은 규제를 적용시킨다.

다중 공선성은 다음의 특징을 가지는데

* 계수가 불안정하다. 작은 변화가 모델의 계수에 큰 영향을 미친다.
* 통계적검증, 모델해석이 어렵다. 특정 독립변수가 종속변수에 미치는 영향을 평가하기 어렵다.



### 사례

아파트 가격데이터를 사용하여 설명해보자면 다음의 변수가 존재한다고 가정했을 때

* 아파트 크기(X1)
* 방의 수(X2)
* 층 수(X3)

이제, X1, X2, X3가 서로 강한 상관관계를 가진다고 가정하자. 예를들어, 아파트가 크면 방의수가 많거나, 층수가 높은 경향이 생긴다.

이러한 상황에서 다중공선성이 발생한다. 





## 회귀 평가 지표

회귀 모델의 성능을 평가하기 위해 사용되는 주요 평가 지표들은 다양하게 있다. 

* **평균 제곱 오차 (Mean Squared Error, MSE)**
* **평균 절대 오차 (Mean Absolute Error, MAE)**
* **평균 제곱근 오차 (Root Mean Squared Error, RMSE)** - MSE 값은 오류의 제곱을 구하므로 실제 오류 평균보다  더 커지므로 루트를 씌웠다.
* **평균 제곱근 로그 오차 (Root Mean Squared Log Error, RMSE)** - RMSE에 로그를 적용한 지표.
* **결정 계수 (R-squared, R^2)** - 1에 가까울 수록 예측 정확도가 높다.



### RMSE와 MSE를 비교했을 때 RMSE가 가지는 특징

* **단위 일치** : RMSE는 실제 값과 동일한 단위를 가지며, 이는 예측 오차의 크기를 실제 값과 동일한 척도로 표현할 수 있게 한다. MSE는 오차를 제곱하므로 실제 값과 다른 단위를 가지게 된다.

* **큰 오차에 높은 패널티 부여**:  크기가 큰 오차에 더 민감하게 반응하므로 큰 오차에 높은 페널티를 부여하는 데 사용된다. 예를들어 (10, 10, 20, 30, 1000)의 오차값들이 있을 때

  ```
  RMSE는 Root((100 + 100 + 400 + 900 + 1000000)/5) = 
  MSE는 (100 + 100 + 400 + 900 + 1000000)/5
  ```

  이 평균값은 1000000에 의해 크게 영향을 받아 상대적으로 큰 값이된다. 따라서 이 식에 제곱근을 취하여 오차의 크기에 대한 패널티를 부여한다.



### RMSLE와 RMSE를 비교했을 때, RMSLE가 가지는 특징

* **이상치에 강건** : RMSLE는 로그를 취하기 때문에 이상치의 영향을 감소시킨다. 

* **작은 오차에 높은 패널티 부여** : 예측값이 실제값보다 클 때보다 예측값이 실제값보다 작을 때 더 큰 패널티를 부여한다. 예를들어 실제 값이 100이고 예측 값이 50일 때 오차는 

  ```
  RMSLE는 log(1 + 100) - log(1 + 50)
  ```

  으로 계산된다.  우항 log(1 + 50)은 log식에의해 분모가 되므로 값이 작을 수록 RMSLE는 커진다.



## 사이킷런 회귀 API 유의사항

```python
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression

model = LinearRegression()

np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')
scores
```

```
array([-0.60767295, -0.79364717, -1.06638027, -0.82413799, -0.97554375])
```

cross_val_score, GridSearchCV와 같은 Scoring함수에 회귀 평가 지표를 적용시 유의사항이 있다.

scoring에 접두사 neg를 붙여 음수값을 반환시키는 이유는, 사이킷런의 Scoring함수가 score 값이 클수록 좋은 평가 결과로 인식하기 때문이다.

회귀 평가 지표에서 Scoring 함수는 오류를 최소화해야한다. 

```
10 > 1 ===> -10 < -1
```

작은 값이 더 큰 숫자로 인식하게 하기위해서는 음수로 변환하면 된다.



## 선형회귀를 위한 데이터 변환방법

1. **타겟값 변환**: Skew 되어있는 경우 주로 Log변환을 적용. 정규분포형태면 좋음
2. **Scaling**: 특성들에 대한 균일한 스케일링/정규화 적용 - StandardScaler를 이용하여 표준 정규분포 형태 변환, 또는 MinMaxScaler를 이용하여 최솟값 0, 최댓값 1로 변환
3. **피쳐값 변환 - 다항 특성변환** : 2번을 완료 후 전처리한 데이터세트에 다시 다항 특성(Polynomial Feature)을 적용하여 변환 - 복잡한 수식이기에 과적합에 유의해야함
4. **피쳐값 변환 - 로그 변환** : 한쪽으로 쏠린 데이터분포에서 주로 사용. 

