# ì•„í‚¤í…ì²˜

ì¸ì½”ë”-ë””ì½”ë”ë¡œ êµ¬ì„±ëœë‹¤.



### íŠ¹ì§•

* Positional Embedding
* layer stack



### ì¸ì½”ë” ìœ í˜•

* Bidirectional attention



### ë””ì½”ë” ìœ í˜•

* Causal attention
* autoregressive attention



### QKV ì‰½ê²Œ ì´í•´í•˜ê¸°

ì €ë…ì‹ì‚¬ì— í•„ìš”í•œ ì¬ë£Œë¥¼ ì‚¬ëŸ¬ ë§ˆíŠ¸ì— ê°”ë‹¤. í•„ìš”í•œ ì‹ì¬ë£Œë¥¼ ê°ê° ì¿¼ë¦¬ë¡œ ê°€ì •í•œë‹¤.

ë§ˆíŠ¸ ì§„ì—´ëŒ€ì— ë¶™ì€ ì´ë¦„í‘œ(í‚¤)ë¥¼ í›‘ìœ¼ë©´ì„œ í•„ìš”í•œ ì¬ë£Œì™€ ì¼ì¹˜(ìœ ì‚¬ë„ í•¨ìˆ˜)í•˜ëŠ”ì§€ í™•ì¸í•œë‹¤.

ì´ë¦„í‘œê°€ ì¼ì¹˜í•˜ë©´ ì§„ì—´ëŒ€ì—ì„œ ìƒí’ˆ(ê°’)ì„ êº¼ë‚¸ë‹¤.

ì…€í”„ ì–´í…ì…˜ì—ì„œëŠ” ë” ì¶”ìƒì ì´ê³  ìœ ì—°í•˜ë‹¤. ì„¤ëª…í•˜ìë©´, í‚¤ì™€ ì¿¼ë¦¬ì˜ ì¼ì¹˜ ì •ë„ì— ë”°ë¼ ë§ˆíŠ¸ì˜ ëª¨ë“  ì´ë¦„í‘œê°€ ì¬ë£Œì— ì¼ì¹˜í•œë‹¤. ë‹¬ê±€ 12ê°œë¥¼ ì‚¬ë ¤ í–ˆì§€ë§Œ, ë‹¬ê±€ 10ê°œì™€ ì˜¤ë¯ˆë › 1ê°œ, ì¹˜í‚¨ ìœ™1ê°œê°€ ì„ íƒë  ë•Œë„ ìˆë‹¤.



### Load model (checkpoint)

```python
from transformers import AutoTokenizer
from bertviz.transformers_neuron_view import BertModel
from bertviz.neuron_view import show

model_ckpt = 'bert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = BertModel.from_pretrained(model_ckpt)
text= 'time flies like an arrow'
show(model, 'bert', tokenizer, text, display_mode='light', layer=0, head=8)
```



### tokenizer

```python
inputs = tokenizer(text, return_tensors='pt', add_special_tokens=False)
inputs.input_ids
```

```
tensor([[ 2051, 10029,  2066,  2019,  8612]])
```

í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì €. ê° í† í°ì€ Tokenizer ì–´íœ˜ì‚¬ì „ì—ì„œ ê° ê³ ìœ í•œ IDì— ë§¤í•‘ëœë‹¤.

add_special_tokens = FalseëŠ” [CLS]ì™€ [SEP] í† í°ì„ ì œì™¸í•œë‹¤ëŠ” ëœ»ì´ë©° ê°„ë‹¨í•˜ê²Œ í•˜ê¸° ìœ„í•¨ì´ë‹¤.



### Load Config & Dense Embedding

ì²´í¬í¬ì¸íŠ¸ì™€ ê´€ë ¨ëœ Config.jsonì„ ë¡œë“œí•œë‹¤.

```python
from torch import nn
from transformers import AutoConfig

config = AutoConfig.from_pretrained(model_ckpt)
token_emb = nn.Embedding(config.vocab_size, config.hidden_size)
token_emb
```

```
Embedding(30522, 768)
```

ğŸ¤— íŠ¸ëœìŠ¤í¬ë¨¸ìŠ¤ì—ì„œ ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ëŠ” vocab_sizeì™€ hidden_size ê°™ì€ ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì§€ì •ëœ ì„¤ì • íŒŒì¼ì´ í• ë‹¹ëœë‹¤. 

ìœ„ ê²½ìš°ì—ëŠ”, ì…ë ¥ IDê°€ 30522ê°œ ì„ë² ë”© ë²¡í„° ì¤‘ í•˜ë‚˜ì— ë§¤í•‘ë˜ê³ , ê·¸ ë²¡í„°ì˜ í¬ê¸°ëŠ” 768ì´ë‹¤.



### Lookup Table

token_emb ê°€ ëœë‹¤.

```python
inputs_embeds = token_emb(inputs.input_ids)
inputs_embeds.size()
```

```
torch.Size([1, 5, 768])
```

[batch_size, seq_len, hidden_dim] í…ì„œë¥¼ ì¶œë ¥í•œë‹¤. ë‹¤ìŒ ë‹¨ê³„ëŠ” QKVì™€ ìœ ì‚¬ë„ë¥¼ ì´ìš©í•œë‹¤.



### ê³µì‹ Scaled dot product attention formula

```
Attention(Q, K, V) = softmax( Q.Dot(K.Transpose()) / sqrt(d_k)) .dot(V)
```



### Calc Scores

```python
import torch
from math import sqrt

query = key = value = inputs_embeds
dim_k = key.size(-1)
scores = torch.bmm(query, key.transpose(1, 2) / sqrt(dim_k))
scores.size()
```

```
torch.Size([1, 5, 5])
```

QKV ë²¡í„°ë¥¼ ë§Œë“¤ê³  ì ê³±ì„ ìœ ì‚¬ë„ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ì–´í…ì…˜ ì ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤.

ë°°ì¹˜ì— ìˆëŠ” ìƒ˜í”Œ ë§ˆë‹¤ 5x5 í¬ê¸°ì˜ Attention Score Matrixê°€ ìƒì„±ëœë‹¤.

ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•˜ê²Œ ëª¨ë‘ì˜ ê°’ì„ ì¼ì¹˜ì‹œì¼°ë‹¤. Scaled dot product attentionì—ì„œ ì ê³±ì€ ì„ë² ë”© ë²¡í„°ì˜ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ì„ ì¡°ì •í•œë‹¤.



### BMM

torch.bmm()ì€ **Batch Matrix-Matrix product**ì˜ ì¤„ì„ë§ë¡œ í¬ê¸°ê°€ [batch_size, seq_len, hidden_dim]ì¸ Queryë²¡í„°ì™€ - Keyë²¡í„°ì˜ ì–´í…ì…˜ ì ìˆ˜ ê³„ì‚°ì„ ë‹¨ìˆœí™”í•œë‹¤. ì•„ë˜ëŠ” ìˆœì„œ. ì¼ë‹¨ ë°°ì¹˜ ì°¨ì›ì„ ë¬´ì‹œí•˜ê³ .

* Key tensorë¥¼ ì „ì¹˜í•˜ì—¬ [hidden_dim, seq_len] í¬ê¸°ë¡œ ë§Œë“ ë‹¤.
* Query ë²¡í„°ì™€ì˜ ì ê³± => [seq_len, hidden_dim].dot(hidden_dim, seq_len) => [seq, seq] í¬ê¸°ì˜ í–‰ë ¬ ìƒì„±



### Softmax ì ìš©

```python
import torch.nn.functional as F

weights = F.softmax(scores, dim=-1)
weights.sum(dim=-1)
```

```
tensor([[1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)
```



```python
attn_outputs = torch.bmm(weights, value)
attn_outputs.shape
```

```
torch.Size([1, 5, 768])
```



### Function

```python
def scaled_dot_product_attention(query, key, value):
    dim_k = query.size(-1)
    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)
    weights = F.softmax(scores, dim=-1)
    return torch.bmm(weights, value)
```





### Multi-Head Attention

ì—¬ëŸ¬ê°œì˜ Attentionì„ ë³‘ë ¬ë¡œ ì‚¬ìš©í•œ í›„ Attention Headë¥¼ ì—°ê²° -> ë‹¤ë¥¸ ì‹œê°ìœ¼ë¡œ ë‹¨ì–´ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•¨. ë¨¼ì € AttentionHead í´ë˜ìŠ¤ë¥¼ ì •ì˜í•œë‹¤.

```python
class AttentionHead(nn.Module):
    def __init__(self, embed_dim, head_dim):
        super().__init__()
        self.q = nn.Linear(embed_dim, head_dim) # í•™ìŠµì˜ ëŒ€ìƒ - ê°€ì¤‘ì¹˜
        self.k = nn.Linear(embed_dim, head_dim)
        self.v = nn.Linear(embed_dim, head_dim)
    
    def forward(self, hidden_state):
        attn_outputs = scaled_dot_product_attention(
            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state)
        )
        return attn_outputs
```

self.q(hidden_state) í‘œí˜„ì€ hidden_stateì˜ ê° ë²¡í„°ë¥¼ Queryë²¡í„°ë¡œ ë³€í™˜í•œë‹¤.

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        embed_dim = config.hidden_size
        num_heads = config.num_attention_heads
        head_dim = embed_dim // num_heads
        self.heads = nn.ModuleList(
            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]
        )
        self.output_linear = nn.Linear(embed_dim, embed_dim)

    def forward(self, hidden_state):
        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)
        x = self.output_linear(x)
        return x # (batch, seq_len, hidden_dim)
```



```python
multihead_attn = MultiHeadAttention(config)
attn_output = multihead_attn(inputs_embeds)
attn_output.size()
```

```
torch.Size([1, 5, 768])
```



