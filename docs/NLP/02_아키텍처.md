# 아키텍처

인코더-디코더로 구성된다.



### 특징

* Positional Embedding
* layer stack



### 인코더 유형

* Bidirectional attention



### 디코더 유형

* Causal attention
* autoregressive attention



### QKV 쉽게 이해하기

저녁식사에 필요한 재료를 사러 마트에 갔다. 필요한 식재료를 각각 쿼리로 가정한다.

마트 진열대에 붙은 이름표(키)를 훑으면서 필요한 재료와 일치(유사도 함수)하는지 확인한다.

이름표가 일치하면 진열대에서 상품(값)을 꺼낸다.

셀프 어텐션에서는 더 추상적이고 유연하다. 설명하자면, 키와 쿼리의 일치 정도에 따라 마트의 모든 이름표가 재료에 일치한다. 달걀 12개를 사려 했지만, 달걀 10개와 오믈렛 1개, 치킨 윙1개가 선택될 때도 있다.



### Load model (checkpoint)

```python
from transformers import AutoTokenizer
from bertviz.transformers_neuron_view import BertModel
from bertviz.neuron_view import show

model_ckpt = 'bert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = BertModel.from_pretrained(model_ckpt)
text= 'time flies like an arrow'
show(model, 'bert', tokenizer, text, display_mode='light', layer=0, head=8)
```



### tokenizer

```python
inputs = tokenizer(text, return_tensors='pt', add_special_tokens=False)
inputs.input_ids
```

```
tensor([[ 2051, 10029,  2066,  2019,  8612]])
```

텍스트 토크나이저. 각 토큰은 Tokenizer 어휘사전에서 각 고유한 ID에 매핑된다.

add_special_tokens = False는 [CLS]와 [SEP] 토큰을 제외한다는 뜻이며 간단하게 하기 위함이다.



### Load Config & Dense Embedding

체크포인트와 관련된 Config.json을 로드한다.

```python
from torch import nn
from transformers import AutoConfig

config = AutoConfig.from_pretrained(model_ckpt)
token_emb = nn.Embedding(config.vocab_size, config.hidden_size)
token_emb
```

```
Embedding(30522, 768)
```

🤗 트랜스포머스에서 모든 체크포인트는 vocab_size와 hidden_size 같은 다양한 하이퍼파라미터가 지정된 설정 파일이 할당된다. 

위 경우에는, 입력 ID가 30522개 임베딩 벡터 중 하나에 매핑되고, 그 벡터의 크기는 768이다.



### Lookup Table

token_emb 가 된다.

```python
inputs_embeds = token_emb(inputs.input_ids)
inputs_embeds.size()
```

```
torch.Size([1, 5, 768])
```

[batch_size, seq_len, hidden_dim] 텐서를 출력한다. 다음 단계는 QKV와 유사도를 이용한다.



### 공식 Scaled dot product attention formula

```
Attention(Q, K, V) = softmax( Q.Dot(K.Transpose()) / sqrt(d_k)) .dot(V)
```



### Calc Scores

```python
import torch
from math import sqrt

query = key = value = inputs_embeds
dim_k = key.size(-1)
scores = torch.bmm(query, key.transpose(1, 2) / sqrt(dim_k))
scores.size()
```

```
torch.Size([1, 5, 5])
```

QKV 벡터를 만들고 점곱을 유사도 함수를 사용해 어텐션 점수를 계산한다.

배치에 있는 샘플 마다 5x5 크기의 Attention Score Matrix가 생성된다.

여기서는 간단하게 모두의 값을 일치시켰다. Scaled dot product attention에서 점곱은 임베딩 벡터의 크기로 스케일을 조정한다.



### BMM

torch.bmm()은 **Batch Matrix-Matrix product**의 줄임말로 크기가 [batch_size, seq_len, hidden_dim]인 Query벡터와 - Key벡터의 어텐션 점수 계산을 단순화한다. 아래는 순서. 일단 배치 차원을 무시하고.

* Key tensor를 전치하여 [hidden_dim, seq_len] 크기로 만든다.
* Query 벡터와의 점곱 => [seq_len, hidden_dim].dot(hidden_dim, seq_len) => [seq, seq] 크기의 행렬 생성



### Softmax 적용

```python
import torch.nn.functional as F

weights = F.softmax(scores, dim=-1)
weights.sum(dim=-1)
```

```
tensor([[1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)
```



```python
attn_outputs = torch.bmm(weights, value)
attn_outputs.shape
```

```
torch.Size([1, 5, 768])
```



### Function

```python
def scaled_dot_product_attention(query, key, value):
    dim_k = query.size(-1)
    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)
    weights = F.softmax(scores, dim=-1)
    return torch.bmm(weights, value)
```





### Multi-Head Attention

여러개의 Attention을 병렬로 사용한 후 Attention Head를 연결 -> 다른 시각으로 단어간의 상관관계를 파악하기 위함. 먼저 AttentionHead 클래스를 정의한다.

```python
class AttentionHead(nn.Module):
    def __init__(self, embed_dim, head_dim):
        super().__init__()
        self.q = nn.Linear(embed_dim, head_dim) # 학습의 대상 - 가중치
        self.k = nn.Linear(embed_dim, head_dim)
        self.v = nn.Linear(embed_dim, head_dim)
    
    def forward(self, hidden_state):
        attn_outputs = scaled_dot_product_attention(
            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state)
        )
        return attn_outputs
```

self.q(hidden_state) 표현은 hidden_state의 각 벡터를 Query벡터로 변환한다.

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        embed_dim = config.hidden_size
        num_heads = config.num_attention_heads
        head_dim = embed_dim // num_heads
        self.heads = nn.ModuleList(
            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]
        )
        self.output_linear = nn.Linear(embed_dim, embed_dim)

    def forward(self, hidden_state):
        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)
        x = self.output_linear(x)
        return x # (batch, seq_len, hidden_dim)
```



```python
multihead_attn = MultiHeadAttention(config)
attn_output = multihead_attn(inputs_embeds)
attn_output.size()
```

```
torch.Size([1, 5, 768])
```



