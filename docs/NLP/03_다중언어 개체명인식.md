문서가 한글, 일본어 등 작성되었다면 

* 허깅페이스 허브에서 적절한 사전학습된 언어 모델을 찾아 작업에 맞게 Fine-tuning	
  * 사전학습에 사용할 웹 텍스트가 많은 언어에만 존재.

말뭉치가 다국어로 된 경우에는?. 

다중언어 트랜스포머 모델이 도움을 준다. 다중언어트랜스포머는 많은 언어로 구성된 대규모 말뭉치를 사전훈련했기 때문에 **Zero-show cross-lingual transfer** 제로샷 교차언어 전이가 가능하다. 한 언어에서 미세튜닝된 모델이 추가 훈련 없이 다른 언어에 적용된다는 의미다. 이런 모델은 한 대화에서 둘 이상의 언어나 사투리를 바꾸는 코드 스위칭에 적합하다.

**제로샷 전이 또는 제로샷 학습** - 트랜스포머에서 제로샷 학습은 GPT-3와 같은 언어모델을 후속 작업에서 미세튜닝하지 않고 평가하는 상황.



## 성능측정

NER 모델 평가는 텍스트 분류 모델 평가와 비슷하다.

정밀도, 재현율,F1 점수의 결과를 관측한다. 유일한 차이는 예측 하나를 정확하다고 판단하기 위해 한 개체명에 있는 모든 단어가 올바르게 예측되어야 한다는 점.